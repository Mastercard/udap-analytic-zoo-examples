{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import argparse\n",
    "import importlib\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "\n",
    "from zoo.orca import init_orca_context, OrcaContext\n",
    "from zoo.orca.learn.tf2.estimator import Estimator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "if os.path.exists('jobs.zip'):\n",
    "    sys.path.insert(0, 'jobs.zip')\n",
    "else:\n",
    "    sys.path.insert(0, './jobs')\n",
    "\n",
    "# load dynamic module\n",
    "ncf_features = importlib.import_module(\"jobs.ncf_features\")\n",
    "ncf_model = importlib.import_module(\"jobs.ncf_model\")\n",
    "\n",
    "\n",
    "__author__ = 'suqiang.song@mastercard.com'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app_name = \"NCF_DL\"\n",
    "    data_source_path = \"/opt/work/data/pcard.csv\"\n",
    "    model_file_name = app_name + '.h5'\n",
    "    save_model_dir = \"/opt/work/model/\"\n",
    "    save_model_file = save_model_dir + model_file_name\n",
    "    u_limit = 10000\n",
    "    m_limit = 200\n",
    "    neg_rate = 5\n",
    "    sliding_length = 1\n",
    "    u_output = 50\n",
    "    m_output = 50\n",
    "    max_epoch = 5\n",
    "    batch_size = 400\n",
    "    predict_output_path = \"/opt/work/output/\"\n",
    "    log_dir = \"/opt/work/logs/\"\n",
    "    train_start = \"201307\"\n",
    "    train_end = \"201401\"\n",
    "    validation_start = \"201402\"\n",
    "    validation_end = \"201403\"\n",
    "    test_start = \"201403\"\n",
    "    test_end = \"201404\"\n",
    "    inference_start = \"201405\"\n",
    "    inference_end = \"201406\"\n",
    "    \n",
    "    \n",
    "    sc = init_orca_context(cluster_mode=\"spark-submit\", init_ray_on_spark=True)\n",
    "    spark = OrcaContext.get_spark_session()\n",
    "\n",
    "    start = time.time()\n",
    "    uDF, mDF, tDF = ncf_features.load_csv(spark,data_source_path,u_limit,m_limit)\n",
    "    trainingDF = ncf_features.genData(tDF,sc,spark,train_start, train_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    #trainingDF.show(5)\n",
    "    validationDF = ncf_features.genData(tDF,sc,spark,validation_start, validation_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    validationDF.show(5)\n",
    "    testDF = ncf_features.genData(tDF,sc,spark,test_start,test_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    #testDF.show(5)\n",
    "    inferenceDF = ncf_features.genData(tDF,sc,spark,inference_start,inference_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    #inferenceDF.show(5)\n",
    "\n",
    "    def get_model(config):\n",
    "        return ncf_model.getKerasModel(u_limit,m_limit,u_output,m_output,log_dir)\n",
    "    est = Estimator.from_keras(model_creator=get_model, model_dir=log_dir)\n",
    "    steps = math.ceil(trainingDF.count() / batch_size)\n",
    "    valid_steps = math.ceil(validationDF.count() / batch_size)\n",
    "    est.fit(data=trainingDF,\n",
    "            batch_size=batch_size,\n",
    "            epochs=max_epoch,\n",
    "            steps_per_epoch=steps,\n",
    "            feature_cols=['features'],\n",
    "            label_cols=['labels'],\n",
    "            validation_data=validationDF,\n",
    "            validation_steps=valid_steps\n",
    "            )\n",
    "    # save checkpoint\n",
    "    est.save(os.path.join(save_model_dir, \"ncf.ckpt\"))\n",
    "    # metrics ,result and save keras model\n",
    "    model = est.get_model()\n",
    "    model.save(save_model_file)\n",
    "    print(model.metrics_names)\n",
    "    #Orca the predict function supports native spark data frame ! Just need to tell batch_size and feature_cols\n",
    "    # use a new Estimamtor to validate load model API\n",
    "    est.load(os.path.join(save_model_dir, \"ncf.ckpt\"))\n",
    "    \n",
    "    #Orca the predict function supports native spark data frame ! Just need to tell batch_size and feature_cols\n",
    "    prediction_df = est.predict(inferenceDF, batch_size=batch_size, feature_cols=['features'])\n",
    "    prediction_df.show(5)\n",
    "    score_udf = udf(lambda pred: 0.0 if pred[0] > pred[1] else 1.0, FloatType())\n",
    "    prediction_df = prediction_df.withColumn('prediction2', score_udf('prediction'))\n",
    "    prediction_df.show(10)\n",
    "    # Save Table\n",
    "    #prediction_final_df.write.mode('overwrite').parquet(predict_output_path)\n",
    "    prediction_df.select('uid','mid','prediction2').write.mode('overwrite').parquet(predict_output_path)\n",
    "    #prediction_df.select('uid','mid','prediction2').write.mode('overwrite').format(\"csv\").save(predict_output_path)\n",
    "    #user_join_df = prediction_df.join(uDF, on=['uid'], how='inner')\n",
    "    #prediction_final_df = user_join_df.join(mDF, on=['mid'], how='inner').select('u','m','prediction').write.mode('overwrite').parquet(predict_output_path)\n",
    "    end = time.time()\n",
    "    print(\"Took time:\"+str((end-start)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
